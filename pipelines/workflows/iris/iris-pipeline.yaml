apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: iris-ml-pipeline
  namespace: pipeline
spec:
  entrypoint: iris-pipeline
  serviceAccountName: pipeline-service-account
  

  # Shared volume between steps
  volumeClaimTemplates:
    - metadata:
        name: workdir
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: gp2
        resources:
          requests:
            storage: 100Mi

  templates:

    # =========================
    # Main DAG Definition
    # =========================
    - name: iris-pipeline
      dag:
        tasks:
          - name: fetch-data
            template: fetch-data-template

          - name: process-data
            template: process-data-template
            dependencies: [fetch-data]

          - name: train-model
            template: train-model-template
            dependencies: [process-data]

    # =========================
    # Step 1: Fetch Dataset
    # =========================
    - name: fetch-data-template
      script:
        image: python:3.11-slim
        command: [python]
        source: |
          import subprocess
          subprocess.run(["pip", "install", "requests", "pandas", "-q"], check=True)

          import requests
          import pandas as pd

          print("STEP 1: FETCHING DATA")

          url = "https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv"
          response = requests.get(url)

          with open("/mnt/data/iris_raw.csv", "w") as f:
              f.write(response.text)

          df = pd.read_csv("/mnt/data/iris_raw.csv")
          print(f"Downloaded {len(df)} records")
        volumeMounts:
          - name: workdir
            mountPath: /mnt/data

    # =========================
    # Step 2: Data Processing
    # =========================
    - name: process-data-template
      script:
        image: python:3.11-slim
        command: [python]
        source: |
          import subprocess
          subprocess.run(["pip", "install", "pandas", "scikit-learn", "-q"], check=True)

          import pandas as pd
          from sklearn.preprocessing import LabelEncoder, StandardScaler

          print("STEP 2: PROCESSING DATA")

          df = pd.read_csv("/mnt/data/iris_raw.csv")

          numeric_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

          # Encode target labels
          le = LabelEncoder()
          df['species_encoded'] = le.fit_transform(df['species'])

          # Normalize numeric features
          scaler = StandardScaler()
          df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

          df.to_csv("/mnt/data/iris_processed.csv", index=False)

          print("Data preprocessing complete.")
        volumeMounts:
          - name: workdir
            mountPath: /mnt/data

    # =========================
    # Step 3: Train Model and Log to MLflow
    # =========================
    - name: train-model-template
      script:
        image: python:3.11-slim
        command: [python]
        source: |
          import subprocess
          subprocess.run([
              "pip", "install",
              "boto3",
              "pandas",
              "scikit-learn",
              "mlflow",
              "-q"
          ], check=True)

          import os
          import pandas as pd
          import mlflow
          import mlflow.sklearn
          from mlflow.tracking import MlflowClient

          from sklearn.model_selection import train_test_split
          from sklearn.linear_model import LogisticRegression
          from sklearn.metrics import accuracy_score

          print("STEP 3: TRAINING MODEL, LOGGING TO MLFLOW, REGISTERING + ALIASING")

          mlflow.set_tracking_uri("http://mlflow.mlflow.svc.cluster.local:5000")
          mlflow.set_experiment("iris-experiment")

          # Registry name + alias you want to point to "the deployable version"
          REGISTERED_MODEL_NAME = os.environ.get("REGISTERED_MODEL_NAME", "iris-logreg")
          DEPLOY_ALIAS = os.environ.get("DEPLOY_ALIAS", "staging")   # e.g. staging / champion / production

          df = pd.read_csv("/mnt/data/iris_processed.csv")
          X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]
          y = df['species_encoded']

          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=0.2, random_state=42
          )

          client = MlflowClient()

          with mlflow.start_run() as run:
              run_id = run.info.run_id

              model = LogisticRegression(max_iter=200)
              model.fit(X_train, y_train)

              y_pred = model.predict(X_test)
              accuracy = accuracy_score(y_test, y_pred)
              print(f"Model accuracy: {accuracy:.4f}")

              mlflow.log_param("model_type", "LogisticRegression")
              mlflow.log_param("max_iter", 200)
              mlflow.log_metric("accuracy", accuracy)

              # Log artifact
              mlflow.sklearn.log_model(model, artifact_path="model")

              # Register from run artifact
              model_uri = f"runs:/{run_id}/model"
              print(f"Registering model: {REGISTERED_MODEL_NAME} from {model_uri}")
              mv = mlflow.register_model(model_uri=model_uri, name=REGISTERED_MODEL_NAME)
              print(f"Registered: name={mv.name}, version={mv.version}")

              # Tags (optional but very "platform" / audit-friendly)
              client.set_model_version_tag(mv.name, mv.version, "metric.accuracy", f"{accuracy:.6f}")
              client.set_model_version_tag(mv.name, mv.version, "pipeline", "iris-argo-workflow")
              client.set_model_version_tag(mv.name, mv.version, "run_id", run_id)

              # Alias (recommended over stages in newer MLflow workflows)
              # This points e.g. "staging" -> latest version you just trained
              print(f"Setting alias '{DEPLOY_ALIAS}' -> version {mv.version}")
              client.set_registered_model_alias(name=mv.name, alias=DEPLOY_ALIAS, version=mv.version)

              print("âœ… Model logged, registered, tagged, and aliased.")
        env:
          - name: REGISTERED_MODEL_NAME
            value: "iris-model"
          - name: DEPLOY_ALIAS
            value: "staging"
        volumeMounts:
          - name: workdir
            mountPath: /mnt/data
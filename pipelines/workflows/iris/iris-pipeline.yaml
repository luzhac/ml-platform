apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: iris-ml-pipeline
  namespace: argo
spec:
  entrypoint: iris-pipeline
  serviceAccountName: argo-workflow

  volumeClaimTemplates:
    - metadata:
        name: workdir
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: gp2
        resources:
          requests:
            storage: 100Mi

  templates:

    # =========================
    # DAG PIPELINE
    # =========================
    - name: iris-pipeline
      dag:
        tasks:
          - name: fetch-data
            template: fetch-data-template

          - name: process-data
            template: process-data-template
            dependencies: [fetch-data]

          - name: train-model
            template: train-model-template
            dependencies: [process-data]

    # =========================
    # STEP 1 - FETCH
    # =========================
    - name: fetch-data-template
      script:
        image: python:3.11-slim
        command: [python]
        source: |
          import subprocess
          subprocess.run(["pip", "install", "requests", "pandas", "-q"], check=True)

          import requests
          import pandas as pd

          url = "https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv"

          print("STEP 1: FETCHING DATA")

          response = requests.get(url)

          with open("/mnt/data/iris_raw.csv", "w") as f:
              f.write(response.text)

          df = pd.read_csv("/mnt/data/iris_raw.csv")
          print(f"Downloaded {len(df)} records")
        volumeMounts:
          - name: workdir
            mountPath: /mnt/data

    # =========================
    # STEP 2 - PROCESS
    # =========================
    - name: process-data-template
      script:
        image: python:3.11-slim
        command: [python]
        source: |
          import subprocess
          subprocess.run(["pip", "install", "pandas", "scikit-learn", "-q"], check=True)

          import pandas as pd
          from sklearn.preprocessing import LabelEncoder, StandardScaler

          print("STEP 2: PROCESSING DATA")

          df = pd.read_csv("/mnt/data/iris_raw.csv")

          numeric_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

          le = LabelEncoder()
          df['species_encoded'] = le.fit_transform(df['species'])

          scaler = StandardScaler()
          df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

          df.to_csv("/mnt/data/iris_processed.csv", index=False)

          print("Data preprocessing complete.")
        volumeMounts:
          - name: workdir
            mountPath: /mnt/data

    # =========================
    # STEP 3 - TRAIN MODEL
    # =========================
    - name: train-model-template
      script:
        image: python:3.11-slim
        command: [python]
        source: |
          import subprocess
          subprocess.run(["pip", "install", "pandas", "scikit-learn", "joblib", "-q"], check=True)

          import pandas as pd
          import joblib
          import json
          from sklearn.model_selection import train_test_split
          from sklearn.linear_model import LogisticRegression
          from sklearn.metrics import accuracy_score

          print("STEP 3: TRAINING MODEL")

          df = pd.read_csv("/mnt/data/iris_processed.csv")

          X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]
          y = df['species_encoded']

          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=0.2, random_state=42
          )

          model = LogisticRegression(max_iter=200)
          model.fit(X_train, y_train)

          y_pred = model.predict(X_test)
          accuracy = accuracy_score(y_test, y_pred)

          print(f"Model accuracy: {accuracy:.4f}")

          # Save model artifact
          joblib.dump(model, "/mnt/data/model.pkl")

          # Save metrics
          metrics = {
              "accuracy": float(accuracy)
          }

          with open("/mnt/data/metrics.json", "w") as f:
              json.dump(metrics, f, indent=2)

          print("Model saved to /mnt/data/model.pkl")
          print("Metrics saved to /mnt/data/metrics.json")
        volumeMounts:
          - name: workdir
            mountPath: /mnt/data
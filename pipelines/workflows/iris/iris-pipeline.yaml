apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  name: iris-ml-pipeline
  namespace: argo
spec:
  entrypoint: iris-pipeline
  serviceAccountName: argo-workflow

  # Shared volume between steps
  volumeClaimTemplates:
    - metadata:
        name: workdir
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: gp2
        resources:
          requests:
            storage: 100Mi

  templates:

    # =========================
    # Main DAG Definition
    # =========================
    - name: iris-pipeline
      dag:
        tasks:
          - name: fetch-data
            template: fetch-data-template

          - name: process-data
            template: process-data-template
            dependencies: [fetch-data]

          - name: train-model
            template: train-model-template
            dependencies: [process-data]

    # =========================
    # Step 1: Fetch Dataset
    # =========================
    - name: fetch-data-template
      script:
        image: python:3.11-slim
        command: [python]
        source: |
          import subprocess
          subprocess.run(["pip", "install", "requests", "pandas", "-q"], check=True)

          import requests
          import pandas as pd

          print("STEP 1: FETCHING DATA")

          url = "https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv"
          response = requests.get(url)

          with open("/mnt/data/iris_raw.csv", "w") as f:
              f.write(response.text)

          df = pd.read_csv("/mnt/data/iris_raw.csv")
          print(f"Downloaded {len(df)} records")
        volumeMounts:
          - name: workdir
            mountPath: /mnt/data

    # =========================
    # Step 2: Data Processing
    # =========================
    - name: process-data-template
      script:
        image: python:3.11-slim
        command: [python]
        source: |
          import subprocess
          subprocess.run(["pip", "install", "pandas", "scikit-learn", "-q"], check=True)

          import pandas as pd
          from sklearn.preprocessing import LabelEncoder, StandardScaler

          print("STEP 2: PROCESSING DATA")

          df = pd.read_csv("/mnt/data/iris_raw.csv")

          numeric_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

          # Encode target labels
          le = LabelEncoder()
          df['species_encoded'] = le.fit_transform(df['species'])

          # Normalize numeric features
          scaler = StandardScaler()
          df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

          df.to_csv("/mnt/data/iris_processed.csv", index=False)

          print("Data preprocessing complete.")
        volumeMounts:
          - name: workdir
            mountPath: /mnt/data

    # =========================
    # Step 3: Train Model and Log to MLflow
    # =========================
    - name: train-model-template
      script:
        image: python:3.11-slim
        command: [python]
        source: |
          import subprocess
          subprocess.run([
              "pip", "install","boto3",
              "pandas",
              "scikit-learn",
              "mlflow",
              "-q"
          ], check=True)

          import pandas as pd
          import mlflow
          import mlflow.sklearn
          from sklearn.model_selection import train_test_split
          from sklearn.linear_model import LogisticRegression
          from sklearn.metrics import accuracy_score

          print("STEP 3: TRAINING MODEL AND LOGGING TO MLFLOW")

          # Set MLflow tracking server (internal cluster DNS)
          mlflow.set_tracking_uri("http://mlflow.mlflow.svc.cluster.local:5000")

          # Create or use experiment
          mlflow.set_experiment("iris-experiment")

          df = pd.read_csv("/mnt/data/iris_processed.csv")

          X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]
          y = df['species_encoded']

          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=0.2, random_state=42
          )

          with mlflow.start_run():

              model = LogisticRegression(max_iter=200)
              model.fit(X_train, y_train)

              y_pred = model.predict(X_test)
              accuracy = accuracy_score(y_test, y_pred)

              print(f"Model accuracy: {accuracy:.4f}")

              # Log parameters
              mlflow.log_param("model_type", "LogisticRegression")
              mlflow.log_param("max_iter", 200)

              # Log metrics
              mlflow.log_metric("accuracy", accuracy)

              # Log model artifact (stored in S3 via MLflow backend)
              mlflow.sklearn.log_model(model, "model")

              print("Model and metrics successfully logged to MLflow.")
        volumeMounts:
          - name: workdir
            mountPath: /mnt/data